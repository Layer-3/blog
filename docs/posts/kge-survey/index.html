<!DOCTYPE html>
<html lang="en"><meta charset="utf-8"><meta name="generator" content="Hugo 0.65.3" /><meta name="viewport" content="width=device-width,initial-scale=1,viewport-fit=cover">
<meta name="color-scheme" content="light dark">
<meta name="supported-color-schemes" content="light dark">

<link rel="icon" type="image/png" sizes="16x16" href="/favicon.png">
<link rel="shortcut icon" href="/favicon.png" type="image/x-icon"><title>知识图谱Embedding综述&nbsp;&ndash;&nbsp;Layer 7</title><link rel="stylesheet" href="/css/core.min.13b7b2b0d13bb838fbc41a20885a38553de7e0fbe92f6c06ab1422828ff749227cb9f9974d563cb370caaa928d3b03c9.css" integrity="sha384-E7eysNE7uDj7xBogiFo4VT3n4PvpL2wGqxQigo/3SSJ8ufmXTVY8s3DKqpKNOwPJ"><body>
    <div class="base-body"><section id="header" class="site header">
    <div class="header wrap"><span class="header left-side"><a class="site home" href="/"><img class="site logo" src="/favicon.png" alt /><span class="site name">Layer 7</span></a></span>
        <span class="header right-side"><div class="nav wrap"><nav class="nav"><a class="nav item" href="/categories/">Categories</a><a class="nav item" href="/tags/">Tags</a><a class="nav item" href="/about/">About</a><a class="nav item" href="https://gohugo.io/"target="_blank">Hugo</a></nav></div></span></div><div class="site slogan"><span class="title">Thanks for layer 3</span></div></section><div id="content"><div class="article-container"><section class="article header">
    <h1 class="article title">知识图谱Embedding综述</h1><p class="article date">Apr 22, 2020<span class="lastmod"> • edited Apr 27, 2020</span></p></section><article class="article markdown-body"><p><em>学习<a href="http://ieeexplore.ieee.org/abstract/document/8047276/"target="_blank">Knowledge Graph Embedding: A Survey of Approaches and Applications</a></em></p>
<h2 id="1-dot-预备知识">1. 预备知识</h2>
<p><em>这部分是我自己在阅读过过程中遇到不懂的地方，自行查阅记录，以供理解</em></p>
<ul>
<li>Embedding:是一个将离散变量转为连续向量表示的一个方式，不光可以减少离散变量的空间维数，同时还可以有意义的表示该变量。上面这句话的意思就是说，把词语拆成指定数量的特征维度，每一个词都可以用这些维度组合成的向量来表示（word embedding 的含义<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>）。相对于稀疏的 one-hot 编码而言，这样的编码可以表示出不同的词语之间的临近关系。</li>
<li>tensor: 张量，在物理学中张量被定义为是一种无关于坐标系的物理量，不需要进行线性（坐标）变换，以统一的方式表达定理的物理量，但是在机器学习中，显然不是这个意思。张量在这里指的是一个多维的数据存储形式，维度被称之为张量的阶(mode)，可以看成向量和矩阵在多维空间的推广，即把向量看作一维张量，矩阵看作二维张量。上述信息和张量的一些运算法则可以参考这篇<a href="http://www.xiongfuli.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/2016-06/tensor-decomposition-part1.html"target="_blank">文章</a></li>
<li>Circular Correlation: 循环相关，对应的是线性相关。相关反应的是同步性、相似性、同相性等，具体的计算方法就是给定位移量 m，计算 x 和 y+m 的对应的点的乘积的和，线性相关的计算公式为： \(r_{xy}(m) = \sum_{n=-\infty}^{+\infty} x(n) y(n+m)\) 。循环相关稍微复杂一点，首先要将上面的有限序列通过向左循环移位得到周期序列（周期就是序列的原长度），然后套用公示进行计算： \(r_{xy}(m) = \sum_{n=0}^{N-1} x(n) y((n+m))_N R_N(n)\) ，计算过程就是循环移位(偏移 m 是多少就移位多少)相乘的乘积和，具体计算过程和理论参考<a href="http://read.pudn.com/downloads70/ebook/254107/ch4.pdf"target="_blank">文章</a>，发现论文中的公式更简单： \([a \star b]_i = \sum_{k=0}^{n-1}[a]_k [b]_{(k+i)~mod~n}\) .
<ul>
<li><em>计算示例</em></li>
</ul>
</li>
</ul>
<span class="image-container"><span class="link" ><a href="https://gitee.com/layer3/pic/raw/master/uPic/20200423213305-screen-shoot.png" 
        target="_blank"><img class="img" src="https://gitee.com/layer3/pic/raw/master/uPic/20200423213305-screen-shoot.png"/></a></span>
</span>
<ul>
<li>超平面(HyperPlanes)，指的是用来切分当前高维空间的切分平面，比如三维空间中的一个分割平面，二维空间中的一条分割线。n维空间中的n-1维的空间。</li>
</ul>
<h2 id="2-dot-介绍">2. 介绍</h2>
<h3 id="2-dot-1-dot-知识图谱">2.1. 知识图谱</h3>
<p>知识图谱就是一个多关系图，由实体和关系组成，每个边都是一个三元组：(head, relation, tail)，这个三元组又被成为<code>fact</code>。</p>
<h3 id="2-dot-2-dot-知识图谱嵌入">2.2. 知识图谱嵌入</h3>
<p>知识图谱（KG）Embedding 是将包含实体和关系的 KG 组件嵌入到连续的向量空间中，以便简化操作，同时保留 KG 的固有结构。</p>
<h3 id="2-dot-3-dot-知识图谱嵌入的通常做法">2.3. 知识图谱嵌入的通常做法</h3>
<p>给定一个 KG，嵌入技术首先使用连续向量空间表示实体和关系，并为每个 fact 定义一个评分函数以衡量其合理性。通过最大化已有的 fact 的得分来获得实体和关系的 Embed。</p>
<blockquote>
<p>但是上述过程学习到的 Embedding 只在针对单个 fact 具有<strong>compatible</strong>，因此不具有<strong>可预测性</strong>。</p>
</blockquote>
<p>因此现在会考虑使用更多信息诸如：<strong>实体类型、关系路径、文本描述、逻辑规则</strong>来让 Embedding 更具有预测性。</p>
<p><strong>具体来说就是只使用 fact 的方法不具有可预测性，因此需要结合其他信息。</strong></p>
<h2 id="3-dot-方法">3. 方法</h2>
<h3 id="3-dot-1-dot-trans系列">3.1. Trans系列</h3>
<ul>
<li>TransE在上篇博文以详细描述，它的score函数为 \(f_r(h,t) = -||h+r-t||_{1/2}\)
<ul>
<li>但是TransE在处理1-N, N-1和 N-N时就会出现问题，比如当同一个实体的某一关系有多个对应时，尽管t的意义可能完全不同，但是TransE也会让他们尽量相似，比如我喜欢狗狗和冰淇淋就会把狗狗和冰淇淋表现的很相似。</li>
</ul>
</li>
<li>为了避免这个问题，一种解决方法是允许实体在不同的关系下有不同的representation，
<ol>
<li>TransH使用<strong>超平面</strong>的方法，relation用r上的法向量为 \(w_r\) 的超平面来表示。给定一个fact (h,r,t)，首先要把h和t投射到超平面上，
<ul>
<li>\(h_{\perp} = h- w_r^{\top}hw_r\) , \(t_{\perp} = t - w_r^{\top}hw_r\)</li>
<li>score函数为：\(f_r(h,t) = - ||h_{\perp} + r - t_{\perp}||_2^2\)</li>
</ul>
</li>
<li>TransR与TransH类似，其为relation定义特定的空间（维度），即定义Entity为d维，但定义relation为k维，对于fact (h,r,t)，先使用投射矩阵 \(M_r\) 把h和t投射到k维上:
<ul>
<li>\(h_{\perp} = M_rh\), \(h_{\perp} = M_rt\)</li>
<li>score函数与TransH相同。</li>
<li>TransR的表达能力很强，但是TransR有个很严重的问题，因为投射矩阵的存在其需要的参数复杂度达到了O(dk)</li>
</ul>
</li>
<li>TransD通过将投射矩阵分解为两个vector $w_r, w_t$，然后根据这两个向量获得投射矩阵 \(M_r^1, M_r^2\)
<ul>
<li>\(M_r^1 = w_rw_h^{\top} + I, M_r^2 = w_rw_t^{\top} + I\)</li>
<li>\(h_{\perp} = M_r^1h, t_{\perp} = M_r^2t\)</li>
</ul>
</li>
<li>TranSparse通过稀疏投射矩阵来减少参数，使用 θ 表示稀疏程度，其有两个版本，分别是用同一个 \(M_r(\theta_r)\) 和用不同的 \(M_r^1(\theta_r^1)h, M_r^2(\theta_r^2)\)
<ul>
<li>\(h_{\perp} = M_r(\theta_r)h\) 或 $h<sub>⊥</sub> = M_r^1(θ_r^1)h$，后面略</li>
</ul>
</li>
</ol>
</li>
<li>另一种解决思路是放宽TransE中 \(h+r \approx t\) 这个条件。
<ol>
<li>TransM给每个fact添加了一个权重θ，为1-N,N-1,N-N分配更小的权重，是的t可以在这些relation中离h+r更远。score函数为：
<ul>
<li>\(f_r(h,t) = -\theta_r ||h+r-t||_{1/2}\)</li>
</ul>
</li>
<li>MoanifoldE放宽条件到 $||h+4-t||_2^2 ≈ θ_r^2$，从而使得t可以在h+r附近的一个圆内，而不是一个点：
<ul>
<li>\(f_r(h,t) = -(||h+r-t||_2^2 - \theta_r^2)^2\)</li>
</ul>
</li>
<li>TransF通过尽让t和h+r在一个方向，同时h和t-r在同一个方向来放松条件：
<ul>
<li>\(f_r(h,t) = (h+r)^{\top}t + (t-r)^{\top}h\)</li>
</ul>
</li>
<li>TransA使用了马氏空间，相比较 TransE 模型，引入了 \(W_r\) 矩阵为不同维度的向量进行加权，并利用 LDL 方法对 \(W_r\) 进行分解:
<ul>
<li>\(f_r(h,t) = -(|h+r-t|)^{\top} M_r (|h+r-t|)\)</li>
</ul>
</li>
</ol>
</li>
</ul>
<section class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1" role="doc-endnote">
<p>参考自<a href="https://www.zhihu.com/question/38002635">https://www.zhihu.com/question/38002635</a> <a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</section>
</article><section class="article labels"><a class="category" href=/categories/study/>Study</a><a class="tag" href=/tags/knowledgegraph/>KnowledgeGraph</a><a class="tag" href=/tags/nlp/>NLP</a></section></div><section class="article navigation"><p><a class="link" href="/posts/transe/"><span class="li">&rarr;</span>TransE，连接多关系实体</a></p></section><section class="article discussion"><div id="disqus_thread"></div>
<script type="application/javascript">
    var disqus_config = function () {
    
    
    
    };
    (function() {
        if (["localhost", "127.0.0.1"].indexOf(window.location.hostname) != -1) {
            document.getElementById('disqus_thread').innerHTML = 'Disqus comments not available by default when the website is previewed locally.';
            return;
        }
        var d = document, s = d.createElement('script'); s.async = true;
        s.src = '//' + "ansont-cn" + '.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="https://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a></section></div><section id="footer" class="footer"><div class="footer-wrap">
    <p class="copyright">©2020 TW.</p>
    <p class="powerby"><span>Powered by </span><a href="https://gohugo.io" 
        target="_blank">Hugo</a><span> and the </span><a href="https://themes.gohugo.io/hugo-notepadium/" 
        target="_blank">Notepadium</a></p>
</div></section><script defer type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-AMS-MML_HTMLorMML" integrity="sha384-e/4/LvThKH1gwzXhdbY2AsjR3rm7LHWyhIG5C0jiRfn8AN2eTN5ILeztWw0H9jmN" crossorigin="anonymous"></script>
        <script
            type="text/x-mathjax-config">MathJax.Hub.Config({ tex2jax: { inlineMath: [['$','$'], ['\\(','\\)']] } });</script>
<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
	ga('create', 'UA-161408542-1', 'auto');
	
	ga('send', 'pageview');
}
</script>
<script async src='https://www.google-analytics.com/analytics.js'></script>
</div>
</body>

</html>